# -*- coding: utf-8 -*-
"""project_Raw_Layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FA8R-HXzs5JWM32gKM9YTa7EFPTW4Fqw

**Raw_layer**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip3 install pyspark

import sys
import time

from pyspark import *
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark.sql.functions import col, when
import logging
import re

from pyspark.sql import SparkSession
class Session:
    spark = SparkSession.builder.appName("log_file_project").config('spark.ui.port', '4050').config("spark.master",
                                                                                                        "local").enableHiveSupport().getOrCreate()
    raw = spark.read.text("/content/drive/MyDrive/updated_ip_log_request.txt")

    

    
    def __init__(self):
        sc = self.spark.sparkContext
        sc.setLogLevel("Error")

    def data_from_s3_bucket(self):
        try:
            self.raw = self.spark.read.text("/content/drive/MyDrive/updated_ip_log_request.txt")
        except Exception as err:
            logging.error('Exception was thrown in connection %s' % err)
            print("Error is {}".format(err))
            sys.exit(1)
        else:
            self.raw.printSchema()
            # raw.show(10, truncate=False)


class RawLayer(Session):
    def extract_data(self):
        regex_pattern = r'(.+?)\ - - \[(.+?)\] \"(.+?)\ (.+?)\ (.+?\/.+?)\" (.+?) (.+?) \"(.+?)\" \"(.+?)\"'

        self.raw = self.raw.withColumn("row_id", monotonically_increasing_id()) \
            .select("row_id", regexp_extract('value', regex_pattern, 1).alias('client/ip'),
                    regexp_extract('value', regex_pattern, 2).alias('datetime'),
                    regexp_extract('value', regex_pattern, 3).alias('method'),
                    regexp_extract('value', regex_pattern, 4).alias('request'),
                    regexp_extract('value', regex_pattern, 6).alias('status_code'),
                    regexp_extract('value', regex_pattern, 7).alias('size'),
                    regexp_extract('value', regex_pattern, 8).alias('referer'),
                    regexp_extract('value', regex_pattern, 9).alias('user_agent'))
        #self.raw.show()

    def remove_special_character(self):
        # Removing special characters from data like ?,/,+,=
        self.raw = self.raw.withColumn('request', regexp_replace('request', r'[%,|"-&.=?-]', ''))

    def bytes_to_kb(self):
        self.raw = self.raw.withColumn('size', round(self.raw.size / 1024, 2)).withColumnRenamed("size","size_in_KB")
        return self.raw

    def replace_null_with_None(self):
        self.raw = self.raw.select(
            [when(col(c) == "-", "NA").otherwise(col(c)).alias(c) for c in self.raw.columns])
        self.raw.show()
        return self.raw

    def count_null_values(self):
        return self.raw.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in self.raw.columns])

    def save_to_s3(self):
        self.raw.write.csv("/content/drive/MyDrive/project_output_files/raw_layer/", mode="overwrite",header=True)

    def save_to_hive_table(self):
        pass
        # self.raw.write.option("mode","overwrite").saveAsTable('Raw_Data2')
        #self.spark.sql("select count(*) from Raw_Data2")
        


if __name__ == "__main__":
    # Session
    Session = Session()
    try:
        Session.data_from_s3_bucket()
    except Exception as e:
        logging.error('Error at %s', 'Reading from S3 Sink', exc_info=e)
        sys.exit(1)

    # RawLayer
    try:
        raw = RawLayer()
    except Exception as e:
        logging.error('Error at %s', 'Cleaning Object Creation', exc_info=e)
        sys.exit(1)
    try:
        raw.extract_data()
    except Exception as e:
        logging.error('Error at %s', 'extract_column_regex', exc_info=e)
        sys.exit(1)

    try:
        raw.remove_special_character()
    except Exception as e:
        logging.error('Error at %s', 'remove_special_character', exc_info=e)
        sys.exit(1)

    try:
        raw.bytes_to_kb()
    except Exception as e:
        logging.error('Error at %s', 'size_to_kb', exc_info=e)
        sys.exit(1)

    try:
        raw.replace_null_with_None()
    except Exception as e:
        logging.error('Error at %s', 'remove empty string with null', exc_info=e)
        sys.exit(1)

    try:
        raw.count_null_values()
    except Exception as e:
        logging.error('Error at %s', 'count null each column', exc_info=e)
        sys.exit(1)

    try:

        raw.save_to_s3()
        logging.info("Writing  Raw_Layer to S3 Successful.")
    except Exception as e:
        logging.error('Error at %s', 'save_to_s3', exc_info=e)
        sys.exit(1)

    try:
        raw.save_to_hive_table()
        logging.info("storing data into hive tables successfull")
    except Exception as e:
        logging.error('Error at %s', 'save to hive table', exc_info=e)

