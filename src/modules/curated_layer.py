# -*- coding: utf-8 -*-
"""ATUL_project_curated_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zpj44gJ0rW5qntGhwenBfkqbyJz3z7zu
"""

!pip3 install pyspark

import sys
import time

from pyspark import *
from pyspark.sql import *
from pyspark.sql import HiveContext
from pyspark.sql.functions import *
from pyspark.sql.functions import col, when
from pyspark.sql import functions as F
import logging
import re

class Setup:
    spark = SparkSession.builder.appName("log_file_project1").config('spark.ui.port', '4050').config(
        "spark.master", "local").enableHiveSupport().getOrCreate()

    curated_df = spark.read.csv(
        "/content/drive/MyDrive/project_output_files/cleansed_Layer/part-00000-4afa4397-87fb-4a49-b4be-afd62a8c095a-c000.csv", header=True,inferSchema=True)

    # spark = SparkSession.builder.appName("Demo Project").master("yarn").enableHiveSupport().getOrCreate()
    def __init__(self):
        sc = self.spark.sparkContext
        sc.setLogLevel("Error")

    def read_from_s3_clean(self):
        try:
            self.curated_df = self.spark.read.csv("/content/drive/MyDrive/project_output_files/cleansed_Layer/part-00000-4afa4397-87fb-4a49-b4be-afd62a8c095a-c000.csv",header=True,inferSchema=True)
          
        except Exception as err:
            logging.error('Exception was thrown in connection %s' % err)
            print("Error is {}".format(err))
            sys.exit(1)
        else:
            pass


class Curated(Setup):

    def drop_referer(self):
        self.curated_df = self.curated_df.drop("referer")
        print("Final output of curated layer")
        self.curated_df.show()

    def write_to_s3(self):
        self.curated_df.write.csv("/content/drive/MyDrive/project_output_files/curated_layer/", mode="overwrite", header=True)

    def write_to_hive(self):
        self.curated_df.write.saveAsTable("curate_log_details0")



class Agg(Curated):


    # check distinct no. of user device ie ip
    def check_distinct_user(self):
        self.curated_df.select("client/ip").distinct().count()

    # add column hour,Get,Post,Head
    def add_temp_columns(self):
        df_temp = self.curated_df.withColumn("No_get", when(col("method") == "GET", "GET")) \
            .withColumn("No_post", when(col("method") == "POST", "POST")) \
            .withColumn("No_Head", when(col("method") == "HEAD", "HEAD")) \
            .withColumn("day", to_date(col("datetime"))) \
            .withColumn("hour", hour(col("datetime"))) \
            .withColumn("day_hour", concat(col("day"), lit(" "), col("hour")))

        

        
        return df_temp

    # perform aggregation per device
    def agg_per_device(self, df_temp):
        df_agg_per_device = df_temp.select("row_id", "day_hour", "client/ip", "no_get", "no_post", "no_head") \
            .groupBy("day_hour", "client/ip") \
            .agg(count("row_id").alias("count_of_records"),
                 count(col("No_get")).alias("no_get"),
                 count(col("No_post")).alias("no_post"),
                 count(col("No_head")).alias("no_head")) \
            # .orderBy(col("row_id").desc())
        

        df_agg_per_device.createOrReplaceTempView("agg_per_device")
        self.spark.sql("select row_number() over(order by 'day_hour')as id, * from agg_per_device")
        print("aggregation per device hive table")
        df_agg_per_device.show()
        return df_agg_per_device

    # perform aggregation across device
    def agg_across_device(self,df_temp):
        df_agg_across_device = df_temp.select("*") \
            .groupBy("day_hour") \
            .agg(
                count("client/ip").alias("no_of_clients"),
                count(col("No_get")).alias("no_get"),
                count(col("No_post")).alias("no_post"),
                count(col("No_head")).alias("no_head")
                )
        df_agg_across_device.createOrReplaceTempView("agg_across_device")
        self.spark.sql("select row_number() over(order by 'day_hour')as id, * from agg_across_device ")
        print("aggregation across device hive table")
        df_agg_across_device.show()
        return df_agg_across_device

    # write to s3 curated-layer-aggregations
    def write_to_s3_agg(self,df_agg_per_device,df_agg_across_device):
        df_agg_per_device.write.csv("/content/drive/MyDrive/project_output_files/aggregations/per_device/", header=True,
                                    mode='overwrite')
        df_agg_across_device.write.csv("/content/drive/MyDrive/project_output_files/aggregations/across_device/",
                                       header=True, mode='overwrite')

    # write to hive
    def write_to_hive(self,df_agg_per_device,df_agg_across_device):
        # df_agg_per_device.write.saveAsTable('log_agg_per_device8')
        print("count of aggregation per device")
        self.spark.sql("select count(*) from log_agg_per_device8").show()
        
        # df_agg_across_device.write.saveAsTable('log_agg_across_device8')
        print("count of aggregation across device")
        self.spark.sql("select count(*) from log_agg_across_device8").show()
        

  

if __name__ == '__main__':
    try:
        setup = Setup()
    except Exception as e:
        logging.error('Error at %s', 'Setup Object creation', exc_info=e)
        sys.exit(1)

    try:
        setup.read_from_s3_clean()
    except Exception as e:
        logging.error('Error at %s', 'read from s3 clean', exc_info=e)
        sys.exit(1)

    try:
        curated = Curated()
    except Exception as e:
        logging.error('Error at %s', 'curated Object creation', exc_info=e)
        sys.exit(1)

    try:
        curated.drop_referer()
    except Exception as e:
        logging.error('Error at %s', 'drop referer', exc_info=e)
        sys.exit(1)

    try:
        curated.write_to_s3()
    except Exception as e:
        logging.error('Error at %s', 'write to s3', exc_info=e)
        sys.exit(1)
    
    try:
        curated.write_to_hive()
    except Exception as e:
        logging.error('Error at %s', 'write to hive', exc_info=e)
        sys.exit(1)

    # Agg
    try:
        agg = Agg()
    except Exception as e:
        logging.error('Error at %s', 'error creating Object agg', exc_info=e)
        sys.exit(1)

    try:
        agg.check_distinct_user()
    except Exception as e:
        logging.error('Error at %s', 'check distinct user', exc_info=e)
        sys.exit(1)

    try:
        df_temp = agg.add_temp_columns()
    except Exception as e:
        logging.error('Error at %s', 'add_temp_columns', exc_info=e)
        sys.exit(1)

    try:
        df_temp_agg_per_device = agg.agg_per_device(df_temp)
    except Exception as e:
        logging.error('Error at %s', 'agg_per_device', exc_info=e)
        sys.exit(1)

    try:
        df_temp_agg_across_device= agg.agg_across_device(df_temp)
    except Exception as e:
        logging.error('Error at %s', 'agg_per_device', exc_info=e)
        sys.exit(1)

    try:
        agg.write_to_s3_agg(df_temp_agg_per_device,df_temp_agg_across_device)
    except Exception as e:
        logging.error('Error at %s', 'write to s3_agg', exc_info=e)
        sys.exit(1)

    try:
        agg.write_to_hive(df_temp_agg_per_device,df_temp_agg_across_device)
    except Exception as e:
        logging.error('Error at %s', 'write to s3_agg', exc_info=e)
        sys.exit(1)

